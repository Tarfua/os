---
alwaysApply: true
---

# Microkernel OS Architecture Specification

## Document Purpose

This specification defines a next-generation general-purpose operating system with microkernel architecture. The design preserves Unix/Linux ecosystem compatibility while eliminating fundamental security and reliability weaknesses through architectural isolation, capability-based security, and modern memory safety.

---

## Part I: Architectural Principles

### 1. Core Design Objectives

**Security**: System security derives from architecture, not bolt-on mechanisms. Isolation and least privilege are structural properties.

**Fault Isolation**: Component failures must not propagate. A crashed driver, filesystem, or network stack cannot compromise kernel integrity or other services.

**Performance**: The system targets both high-throughput server workloads and low-latency interactive desktop use cases without compromise.

**Hardware Ecosystem**: Driver model designed for long-term vendor support through stable ABIs and clear isolation boundaries.

**Maintainability**: Complexity growth is controlled through explicit boundaries. Innovation occurs in replaceable user-space components, not kernel expansion.

**Compatibility**: POSIX and Linux ABI support provided as optional service layers, not core constraints.

---

### 2. System Architecture Model

```
┌─────────────────────────────────────────────────────────────┐
│                    Application Layer                        │
│  (POSIX apps, native binaries, containers, language runtimes)│
├──────────────────────┬──────────────────────────────────────┤
│   System Services    │      Compatibility Services          │
│ (VFS, network mgr,   │   (POSIX syscall server,             │
│  policy manager)     │    Linux ABI emulation)              │
├──────────────────────┴──────────────────────────────────────┤
│          User-Space Isolated Service Processes              │
│  Filesystem servers | Network stacks | Device drivers       │
│  GPU manager | USB stack | Storage controller | Audio       │
├─────────────────────────────────────────────────────────────┤
│             Microkernel (Minimal Trusted Core)              │
│  Thread scheduler | Virtual memory | IPC mechanism          │
│  Capability system | Interrupt routing | IOMMU control      │
├─────────────────────────────────────────────────────────────┤
│                         Hardware                            │
└─────────────────────────────────────────────────────────────┘
```

---

### 3. Microkernel Responsibilities (Exhaustive List)

The kernel provides only fundamental mechanisms:

**Thread Execution**: Preemptive scheduling with priority inheritance and deadline scheduling support. Per-thread quantum management and CPU affinity control.

**Memory Management**: Virtual address space creation and destruction. Page table management. Physical frame allocation. Memory protection enforcement.

**Inter-Process Communication**: Synchronous message passing with capability transfer. Endpoint-based rendezvous. Zero-copy shared memory setup under strict access control.

**Security Enforcement**: Capability validation on every kernel operation. No operation proceeds without explicit capability authority.

**Hardware Abstraction**: Interrupt delivery to appropriate user-space handlers. IOMMU configuration for DMA isolation. Basic platform initialization.

**Kernel Self-Management**: Internal synchronization primitives. Debug and trace facilities. Panic handling and error reporting.

---

### 4. Explicit Kernel Exclusions

The following functionality explicitly does not belong in kernel space:

- File system implementations of any kind
- Network protocol stacks (TCP/IP, UDP, routing)
- Device drivers (block, character, network, graphics)
- USB and Bluetooth stacks
- Audio and video processing
- Graphics rendering and compositing
- Cryptographic operations (except capability token generation)
- Policy enforcement beyond capability checks
- Dynamic module loading beyond initial boot services

These execute as isolated user-space server processes.

---

### 5. Security Architecture

#### 5.1 Capability-Based Security Model

**No Global Root Authority**: The system has no superuser or administrator with unrestricted access.

**Capability Definition**: A capability is an unforgeable token representing permission to perform a specific operation on a specific kernel object.

**Capability Types**:
- Thread capabilities: control thread lifecycle and scheduling parameters
- AddressSpace capabilities: control memory mappings
- Endpoint capabilities: enable IPC communication
- IRQ capabilities: receive hardware interrupt notifications
- Memory capabilities: grant access to specific physical or device memory regions
- Port capabilities: access hardware I/O ports

**Capability Properties**:
- Unforgeable: cannot be created or guessed, only granted by kernel or delegation
- Fine-grained: each capability grants minimal necessary authority
- Transferable: can be sent to other processes via IPC
- Revocable: issuing authority can invalidate capabilities
- Inspectable: processes can query their own capability set

#### 5.2 Isolation Guarantees

**Address Space Isolation**: Every user-space process executes in a separate address space. Memory cannot be accessed without explicit capability.

**Service Isolation**: Filesystem servers cannot access raw block devices without block device capability. Network stacks cannot access filesystem data without explicit file capability.

**Driver Containment**: Device drivers execute with access only to assigned hardware resources. A compromised driver cannot access other devices or system memory.

**Failure Domains**: Service crash or compromise is contained to that service. System provides mechanisms to detect failures and restart services without global impact.

---

### 6. Driver Architecture

#### 6.1 User-Space Driver Model

**Isolation**: Each driver runs as a separate user-space process with minimal privileges.

**Failure Handling**: Driver crashes do not crash the kernel. System can detect and restart failed drivers while maintaining device state where possible.

**Hardware Access Control**:
- DMA operations require IOMMU mappings explicitly granted via capability
- Memory-mapped I/O regions exposed only through capability-controlled mappings
- I/O port access granted per-port via capability
- Interrupt delivery routed to driver via capability-checked endpoint

#### 6.2 Driver Interface Stability

**Stable Binary ABI**: Driver interface is versioned and maintains backward compatibility across kernel updates.

**Interface Guarantees**:
- IPC protocols for driver-kernel communication are stable
- Hardware abstraction structures maintain binary compatibility
- Capability types and semantics do not change within major versions

**Vendor Support**: Stable ABI enables hardware vendors to ship binary drivers with long-term support commitments.

---

### 7. Filesystem Architecture

#### 7.1 Filesystem as Service

**Service Model**: Each filesystem implementation runs as an isolated user-space server process.

**Multiple Instances**: Different filesystems for different purposes:
- Local persistent filesystems (ext4-like, copy-on-write, log-structured)
- Network filesystems (NFS, SMB client implementations)
- Special-purpose filesystems (encrypted overlay, union, RAM)

**VFS Protocol**: All filesystem servers expose a common typed interface for file operations. This replaces text-based pseudo-filesystems with structured queries.

#### 7.2 Structured System Interface

**No Text Pseudo-Files**: The system does not use `/proc` or `/sys` style text-based configuration.

**Typed System Calls**: System queries return structured data with explicit versioning.

**Backward Compatibility**: Old application versions work because server maintains support for old interface versions.

---

### 8. Network Architecture

#### 8.1 Network Stack Isolation

**User-Space Stacks**: All network protocol implementations run in user space.

**Multiple Stacks**: Different stacks optimized for different requirements:
- High-security stack with extensive validation
- High-performance stack with kernel bypass for trusted applications
- Compatibility stack for legacy protocol support

**Stack Replacement**: Network stacks are replaceable without kernel changes or system reboot.

#### 8.2 Network Security

**Capability-Controlled Access**: Raw packet access requires explicit capability. Applications cannot sniff traffic without authority.

**Socket Service**: User-facing socket API provided by a socket server that mediates between applications and network stacks.

**Firewall Integration**: Packet filtering implemented in user-space with capability checks. No application sends or receives packets without appropriate capabilities.

---

### 9. Inter-Process Communication Model

#### 9.1 IPC as Core Mechanism

**Universal Communication**: All service access occurs via IPC. File operations, device control, network operations, and system management all use the same IPC primitives.

**IPC Operations**:
- Synchronous send and receive with blocking semantics
- Asynchronous notification via capability-granted endpoints
- Shared memory region establishment under mutual consent

**IPC Performance**:
- Optimized fast path for small message passing
- Zero-copy transfer for bulk data via shared memory mapping
- Capability transfer in single IPC transaction

#### 9.2 Service Discovery and Naming

**No Global Namespace**: Services are not identified by string names in a global directory.

**Capability-Based Discovery**: Initial process receives capabilities to essential services. Additional service discovery occurs through queries to trusted service directory with capability proof.

---

### 10. Scheduling and Performance

#### 10.1 Scheduler Design

**Multiple Scheduling Classes**:
- Real-time: deadline-based scheduling for hard real-time requirements
- Interactive: low-latency scheduling for UI responsiveness
- Throughput: fair scheduling for batch and server workloads

**Scheduler Features**:
- Priority inheritance to prevent priority inversion
- CPU affinity control for cache optimization
- Gang scheduling for tightly coupled thread groups

#### 10.2 Performance Optimization Strategy

**Fast Paths**: Critical IPC paths optimized for common case with minimal kernel processing.

**Kernel Bypass**: Trusted applications can use shared memory for data plane operations while control plane remains capability-checked.

**NUMA Awareness**: Scheduler and memory allocator consider NUMA topology for large systems.

**Power Management**: Scheduler cooperates with power management to balance performance and energy efficiency.

---

### 11. Memory Safety Strategy

#### 11.1 Safe Language Implementation

**Rust-Based Kernel**: Kernel implemented in Rust to eliminate entire classes of memory safety vulnerabilities.

**Safety Benefits**:
- No buffer overflows or underflows
- No use-after-free errors
- No data races in kernel code
- Type system enforces initialization and lifetime rules

**Unsafe Code Management**:
- Unsafe Rust limited to well-defined hardware interaction boundaries
- Unsafe blocks explicitly audited and documented
- Assembly limited to architecture-specific boot and context switch code

#### 11.2 User-Space Memory Safety

**Driver Requirements**: New drivers strongly encouraged to use memory-safe languages. Rust driver framework provided.

**Legacy Driver Support**: Drivers in unsafe languages run in isolated processes where memory corruption impact is contained.

**Validation**: Unsafe drivers subject to additional runtime validation and address space restriction.

---

### 12. Compatibility Layer Design

#### 12.1 POSIX Compatibility Service

**Implementation**: POSIX API provided by user-space service, not kernel.

**POSIX Features**:
- Standard syscall interface (open, read, write, fork, exec, etc.)
- Process model with signals
- File descriptors and I/O redirection
- Pipes and FIFOs
- Standard error semantics

**Internal Mapping**: POSIX server translates traditional operations to native capability and IPC mechanisms.

#### 12.2 Linux ABI Compatibility

**Binary Compatibility**: Optional subsystem provides Linux syscall ABI for unmodified Linux binaries.

**Implementation**: Containerized Linux personality with syscall translation layer.

**Use Cases**: Legacy application support during transition period. Not intended as permanent architecture.

---

### 13. System Composition Philosophy

**Minimal Kernel**: Kernel size measured in tens of thousands of lines, not millions. Small enough for formal verification efforts.

**Replaceable Services**: System services can be updated, replaced, or specialized without kernel changes.

**Independent Restart**: Failed services restart without affecting other components or requiring system reboot.

**Observability**: All IPC is observable. Capability flow is traceable. System behavior is introspectable.

---

## Part II: Development Roadmap

### Development Environment and Methodology

#### Primary Development Platform: QEMU

**Rationale**: QEMU provides complete system emulation enabling development without physical hardware dependencies.

**QEMU Capabilities Used**:
- Full x86_64 CPU emulation with modern extensions
- UEFI firmware (OVMF) for realistic boot environment
- Device emulation for drivers (serial, storage, network)
- GDB remote debugging protocol for kernel debugging
- Snapshot and restore for deterministic testing

**Standard Development Command**:
```
qemu-system-x86_64 \
  -machine q35 \
  -cpu qemu64 \
  -m 2G \
  -serial stdio \
  -drive if=pflash,format=raw,readonly=on,file=OVMF_CODE.fd \
  -drive format=raw,file=boot.img \
  -s -S
```

#### Debugging Methodology

**Serial Output**: Primary debug output via serial port, captured to stdio.

**GDB Integration**:
- `-s` flag opens GDB server on port 1234
- `-S` flag halts CPU at startup for early debugging
- Source-level debugging of all kernel code
- Hardware breakpoint and watchpoint support

**Debug Workflow**:
- Set breakpoints in memory manager, scheduler, IPC handler
- Single-step through critical kernel paths
- Inspect data structures and register state
- Validate invariants at runtime

#### Testing Strategy

**Automated Boot Testing**: QEMU instances boot OS, execute test service, report results via serial output.

**Test Categories**:
- Unit tests: IPC semantics, capability enforcement, memory isolation
- Integration tests: Service interaction, filesystem operations, network communication
- Stress tests: Scheduler fairness, memory pressure, IPC throughput
- Regression tests: Previously fixed bugs remain fixed

**Continuous Integration**: Every commit boots in QEMU and executes test suite.

---

### Stage 1: Boot and Hardware Initialization

**Objective**: Establish execution environment capable of running kernel code.

**Components**:

**UEFI Boot Loader**:
- UEFI application entry point
- Load kernel image into memory
- Construct memory map from UEFI
- Exit boot services and transfer control to kernel

**Long Mode Setup**:
- Enable 64-bit execution mode
- Establish initial page tables with higher-half mapping
- Identity map low memory for boot process
- Set up kernel code and data segments

**Interrupt Infrastructure**:
- Initialize Interrupt Descriptor Table with null handlers
- Configure timer interrupt for basic time reference
- Install panic handler for unexpected interrupts

**Early Output**:
- Initialize serial port for debug output
- Implement basic string formatting
- Provide kernel logging macros

**Memory Discovery**:
- Parse UEFI memory map
- Identify usable physical memory regions
- Mark reserved regions (UEFI runtime, hardware MMIO)

**Success Criteria**: Kernel boots, prints "Kernel initialized" to serial console, handles timer interrupts without crashing.

---

### Stage 2A: Memory Management Foundation

**Objective**: Implement fundamental memory allocation and virtual memory mechanisms without involving scheduling or IPC.

**Components**:

**Physical Memory Allocator**:
- Frame allocator managing physical memory pages
- Bitmap or buddy allocator for tracking free frames
- Support for page-aligned allocations
- Statistics tracking (total, used, free frames)

**Virtual Memory Primitives**:
- Page table manipulation functions (map, unmap, remap)
- Support for different page sizes (4KiB, 2MiB, 1GiB)
- Cache and permission attribute setting
- Page table walk and address resolution

**AddressSpace Abstraction**:
- AddressSpace as kernel object encapsulating page table root
- Per-address-space metadata (name, statistics, capability list)
- Creation and destruction operations
- Address space switching function

**Kernel Heap Allocator**:
- Dynamic memory allocation for kernel data structures
- Built on top of physical frame allocator
- Support for variable-size allocations
- Alignment requirements handling

**Isolation Verification**:
- User address spaces cannot map kernel memory
- User address spaces cannot map other user address spaces
- Kernel maintains read-only mapping of all user memory for IPC data copy

**Explicit Exclusions**:
- No demand paging or page fault handling
- No copy-on-write
- No shared memory between address spaces
- No swapping or memory overcommit

**Success Criteria**: Kernel allocates multiple isolated address spaces, switches between them without corruption, and maintains separate mapping state.

---

### Stage 2B: Thread Execution and Scheduling

**Objective**: Enable preemptive multitasking with timer-driven scheduling. No communication between threads yet.

**Components**:

**Thread Abstraction**:
- Thread structure containing register context, stack pointers, and execution state
- Association with specific AddressSpace
- Per-thread kernel stack
- Thread identifier and metadata

**Thread States**:
- Running: currently executing on CPU
- Runnable: ready to execute, waiting for CPU
- Blocked: waiting for event (used in next stage)
- Dead: terminated and awaiting cleanup

**Context Switch Mechanism**:
- Save full CPU register state (general purpose, stack pointer, instruction pointer)
- Restore register state for next thread
- Page table switch when crossing address space boundary
- Return to user mode via `iretq` or equivalent

**Scheduler Core**:
- Single run queue holding all runnable threads
- Round-robin scheduling policy with configurable quantum
- Select next thread when current quantum expires or thread yields
- Handle thread state transitions

**Timer Integration**:
- Configure hardware timer for periodic interrupts
- Timer interrupt handler invokes scheduler
- Preempt running thread and switch to next runnable

**Preemption Control**:
- Kernel critical sections can disable preemption
- Preemption disabled counter per CPU
- Scheduler defers decisions until preemption re-enabled

**Explicit Exclusions**:
- No priority levels
- No real-time scheduling
- No SMP (single CPU only)
- No thread sleep or wakeup (no blocking primitives)
- No thread-local storage

**Success Criteria**: Multiple threads execute in rotation, each in isolated address space. Threads preempted by timer demonstrate fair CPU sharing.

---

### Stage 2C: IPC and Capability Foundation

**Objective**: Enable controlled communication and authority transfer between isolated processes.

**Components**:

**Endpoint Object**:
- Kernel object representing IPC rendezvous point
- Holds at most one waiting sender and one waiting receiver
- Manages thread blocking and wakeup on IPC match

**IPC Message Structure**:
- Fixed-size payload (eight 64-bit words)
- Inline capability transfer slots (up to four capabilities per message)
- No variable-length data or shared memory in initial implementation

**IPC Operations**:

**Send Operation**:
- Caller specifies endpoint capability, message data, and capabilities to transfer
- If receiver waiting: deliver immediately, unblock receiver, transfer capabilities
- If no receiver: block sender until receiver arrives

**Receive Operation**:
- Caller specifies endpoint capability to receive from
- If sender waiting: extract message, unblock sender, receive transferred capabilities
- If no sender: block receiver until sender arrives

**Thread Blocking Integration**:
- Sending thread enters Blocked state when no receiver
- Receiving thread enters Blocked state when no sender
- Matched send/receive operations atomically unblock both parties

**Capability System**:

**Capability Types**:
- Thread: control thread execution and query state
- AddressSpace: control memory mappings
- Endpoint: send and receive IPC messages
- IRQ (placeholder for future): receive interrupt notifications

**Capability Storage**:
- Per-thread capability table indexed by integer capability identifiers
- Kernel validates capability index and type on every operation
- Invalid capability index or type mismatch causes operation failure

**Capability Transfer**:
- Capabilities embedded in IPC messages
- Sender includes capability in outgoing message
- Receiver gains new capability in its capability table
- Original sender retains capability (copy semantics)

**Capability Operations**:
- Acquire: receive capability via IPC
- Drop: release capability from capability table
- Inspect: query capability type (future: query authority level)

**Security Properties**:
- No ambient authority: operations require explicit capability
- Capability unforgeable: cannot be created outside kernel control
- Least privilege: each thread holds only necessary capabilities

**Explicit Exclusions**:
- No asynchronous IPC or notification mechanisms
- No shared memory establishment
- No capability revocation or delegation hierarchy
- No capability grant (only transfer via IPC)
- No variable-length message payloads

**Success Criteria**: Two processes communicate via IPC. Sender transfers endpoint capability to receiver. Receiver uses transferred capability to communicate with third process.

---

### Stage 2 Summary

At completion of Stage 2:

**Kernel Provides**:
- Isolated address spaces with independent virtual memory
- Preemptive thread scheduling with fair CPU sharing
- Synchronous IPC as sole inter-process communication mechanism
- Capability-based access control on all kernel operations

**What Remains For Later Stages**:
- Shared memory for bulk data transfer
- Asynchronous notifications
- Filesystem services
- Device drivers
- Network stacks
- SMP support
- Advanced scheduling policies
- Capability revocation and hierarchical delegation

This foundation is minimal, correct, and sufficient for building complete OS services in user space.

---

### Stage 3: Shared Memory and Zero-Copy IPC

**Objective**: Enable efficient bulk data transfer between processes while maintaining isolation guarantees.

**Components**:

**Shared Memory Regions**:
- Kernel object representing memory region accessible to multiple address spaces
- Created by explicit process request with size and access permissions
- Memory allocated from physical frames on creation

**Mapping Protocol**:
- Process A creates shared memory region with specified size
- Process A receives capability to shared memory object
- Process A transfers capability to Process B via IPC
- Process B maps shared memory into its address space at chosen virtual address
- Both processes access same physical memory

**Access Control**:
- Shared memory capability specifies permissions (read-only, read-write)
- Kernel enforces permissions on mapping operation
- Process cannot escalate privileges on shared memory region

**Lifecycle Management**:
- Shared memory persists while any capability exists
- Last capability release triggers memory deallocation
- Explicit unmap operation removes mapping from address space

**Zero-Copy IPC Pattern**:
- Sender allocates shared memory, writes data
- Sender sends small IPC message with shared memory capability
- Receiver maps shared memory, reads data
- Receiver unmaps when done, drops capability

**Success Criteria**: Large data transfer (megabytes) between processes completes with minimal kernel involvement. Memory isolation verified: corrupting shared region does not affect process private memory.

---

### Stage 4: System Services Foundation

**Objective**: Establish core user-space services that provide system-wide functionality.

**Components**:

**Root Service**:
- First user-space process started by kernel
- Receives capabilities to all system resources
- Responsible for starting other system services
- Distributes capabilities according to system policy

**Process Manager Service**:
- Handles process creation and termination
- Manages process hierarchy and parent-child relationships
- Provides process enumeration and query interface
- Enforces resource limits and quotas

**Memory Manager Service**:
- Implements memory allocation policy
- Provides malloc/free equivalent for processes
- Manages backed memory (file-mapped pages)
- Tracks system-wide memory usage

**Capability Manager Service**:
- Maintains capability namespace
- Implements capability revocation
- Enables capability delegation with restrictions
- Provides capability audit and introspection

**Name Service**:
- Maps human-readable names to endpoint capabilities
- Allows service discovery by name
- Enforces access control on name registration and lookup
- Replaces global namespace with capability-controlled directory

**Success Criteria**: Process creation goes through Process Manager. Memory allocation requests handled by Memory Manager. Services discover each other via Name Service.

---

### Stage 5: Interrupt Handling and Driver Framework

**Objective**: Enable hardware interaction through isolated user-space drivers.

**Components**:

**Interrupt Delivery**:
- Hardware interrupts generate IRQ events
- Kernel identifies process registered for IRQ
- Kernel sends IPC message to driver process notifying of interrupt
- Driver process handles interrupt, returns to waiting state

**IRQ Capability**:
- Driver receives IRQ capability during initialization
- IRQ capability grants right to receive interrupt notifications
- Multiple drivers cannot share same IRQ without explicit coordination

**IOMMU Integration**:
- Kernel configures IOMMU to restrict DMA to permitted memory regions
- Driver receives memory capability specifying DMA-accessible region
- Hardware can only DMA to/from designated region
- Memory outside region protected from device access

**Driver Initialization Protocol**:
- Root service starts driver process
- Driver registers with device manager
- Device manager grants driver capabilities: MMIO region, IRQ, DMA memory
- Driver configures hardware and enters event loop

**Driver Event Loop Pattern**:
- Wait for IPC message (interrupt notification or control command)
- Process event
- Respond if necessary
- Return to waiting

**First Reference Drivers**:
- Serial driver: character device using interrupt-driven I/O
- Timer driver: periodic interrupt generation for time-of-day
- RAM disk driver: block device backed by memory

**Success Criteria**: Serial driver receives interrupts from hardware, delivers characters to console service. Driver crash does not crash kernel. Restarted driver successfully resumes operation.

---

### Stage 6: Filesystem Service

**Objective**: Provide persistent storage abstraction entirely in user space.

**Components**:

**VFS Protocol**:
- Typed IPC interface for filesystem operations
- Operations: open, close, read, write, seek, stat, readdir
- Path resolution and directory traversal
- File handle representation via capabilities

**RAM Filesystem Implementation**:
- In-memory filesystem for testing and temporary storage
- Directory tree stored in process memory
- File data stored in allocated memory
- No persistence across restarts

**Block Device Integration**:
- Filesystem server receives capability to block device driver endpoint
- Read and write requests sent to block device via IPC
- Block device returns data via shared memory

**Persistent Filesystem**:
- Simple on-disk format (similar to ext2)
- Inode-based structure with directory and data blocks
- Metadata stored in fixed locations
- Crash recovery through journaling or fsck equivalent

**Mount Protocol**:
- VFS service manages multiple filesystem instances
- Applications send requests to VFS
- VFS forwards to appropriate filesystem implementation
- Transparent access to different storage backends

**Success Criteria**: Application creates file, writes data, closes file. Second application opens same file, reads back data. Filesystem server restart preserves existing file data.

---

### Stage 7: Network Stack

**Objective**: Provide network communication through isolated user-space protocol stack.

**Components**:

**NIC Driver**:
- User-space driver for network interface card
- Receives packet buffers from NIC via DMA
- Delivers received packets to network stack via IPC
- Transmits packets from network stack to NIC

**Protocol Stack Implementation**:
- User-space TCP/IP stack (port of lwIP or custom implementation)
- Handles Ethernet, IP, ICMP, UDP, TCP protocols
- Maintains connection state and routing tables
- Runs as isolated service process

**Socket Service**:
- Provides BSD socket API to applications
- Translates socket calls to IPC messages to protocol stack
- Manages file descriptor namespace for sockets
- Enforces network access policy via capabilities

**Capability-Controlled Access**:
- Network transmit capability required to send packets
- Network receive capability required to receive packets
- Port binding capability required to listen on port
- Raw socket capability tightly restricted

**Zero-Copy Packet Path**:
- NIC driver and protocol stack share packet buffers
- Application and protocol stack share send/receive buffers
- Minimize copying for bulk data transfer

**Success Criteria**: Application opens TCP connection, transfers data, closes connection. Connection survives protocol stack restart (with state migration). Multiple applications communicate simultaneously.

---

### Stage 8: POSIX Compatibility Layer

**Objective**: Enable existing Unix applications to run on microkernel OS.

**Components**:

**POSIX Server**:
- User-space service implementing POSIX syscall interface
- Receives syscall requests via IPC
- Translates to native microkernel operations
- Returns results to application

**Syscall Interception**:
- Special loader for POSIX applications
- Redirects syscall instructions to IPC messages
- Transparent to application code
- No kernel modification required

**Process Model Emulation**:
- Fork/exec semantics implemented via process creation IPC
- Process group and session management
- Signal delivery via asynchronous IPC
- Wait/waitpid through process termination notification

**File Descriptor Mapping**:
- File descriptors map to VFS file capabilities
- Standard streams (stdin, stdout, stderr) established at process creation
- Pipe and FIFO support through shared memory and synchronization

**Filesystem Path Translation**:
- POSIX paths mapped to VFS operations
- Current working directory maintained per process
- Path resolution implemented in POSIX server

**Success Criteria**: Standard Unix utilities (ls, cat, grep, shell) run without modification. Shell scripts execute. Simple development tools (compiler, linker) function correctly.

---

### Stage 9: Multi-Core Support

**Objective**: Utilize multiple CPU cores while maintaining isolation guarantees.

**Components**:

**Per-CPU Structures**:
- Each CPU has dedicated kernel stack
- Per-CPU current thread pointer
- Per-CPU scheduler run queue
- Per-CPU interrupt handling state

**Bootstrap Protocol**:
- Boot processor initializes kernel
- Boot processor starts application processors
- Each application processor initializes per-CPU state
- Application processors enter scheduler

**Scheduler Extensions**:
- Load balancing between CPU run queues
- Thread migration for work distribution
- CPU affinity support for cache optimization
- Preemption across all CPUs

**Synchronization Primitives**:
- Spinlocks for short critical sections
- Read-write locks for reader-heavy workloads
- Lock-free data structures where feasible
- Deadlock detection in development builds

**IPC Across Cores**:
- Endpoint locking prevents race conditions
- Thread blocking and wakeup handled atomically
- Capability transfer synchronized correctly

**Success Criteria**: All CPU cores execute threads. Work distributed across cores. System remains stable under high load on all cores.

---

### Stage 10: Advanced Scheduling and Real-Time

**Objective**: Support diverse workload requirements with sophisticated scheduling.

**Components**:

**Scheduling Classes**:
- Real-time: fixed-priority preemptive scheduling
- Deadline: earliest deadline first scheduling
- Fair: CPU time distributed proportionally
- Interactive: low-latency for user-facing threads

**Per-Thread Scheduling Parameters**:
- Priority level within scheduling class
- CPU affinity mask
- Time quantum
- Deadline parameters (period, execution time)

**Priority Inheritance**:
- Detect priority inversion situations
- Temporarily boost low-priority thread holding lock
- Restore original priority when lock released

**Group Scheduling**:
- Threads organized into scheduling groups
- CPU time distributed between groups first, then within group
- Supports container isolation and resource control

**Success Criteria**: Audio processing thread meets real-time deadline consistently. Interactive UI thread maintains low latency under background load. Batch processing receives fair share without starving interactive work.

---

### Stage 11: Advanced Filesystem Features

**Objective**: Provide modern filesystem capabilities beyond basic storage.

**Components**:

**Copy-on-Write Filesystem**:
- Implement CoW semantics for efficient snapshots
- Block sharing between files and snapshots
- Space-efficient cloning of large files

**Encryption Layer**:
- Transparent encryption of file data
- Key management via capability-controlled service
- Per-file or per-directory encryption policies

**Network Filesystems**:
- NFS client implementation
- SMB/CIFS client for Windows compatibility
- Remote filesystem access via standard VFS interface

**Union/Overlay Filesystems**:
- Combine multiple filesystems into single view
- Support for read-only base layer with writable overlay
- Container and package management applications

**Success Criteria**: Create filesystem snapshot instantly. Modify files without affecting snapshot. Encrypted filesystem survives key rotation. Network filesystem performs comparably to local storage.

---

### Stage 12: Container and Virtualization Support

**Objective**: Enable lightweight isolation for application deployment.

**Components**:

**Namespace Isolation**:
- Process ID namespace per container
- Filesystem namespace (chroot equivalent)
- Network namespace with separate stack instance
- IPC namespace with isolated endpoint space

**Resource Limits**:
- CPU time limits per container
- Memory usage limits enforced by memory manager
- Network bandwidth limits
- Disk I/O rate limits

**Container Lifecycle**:
- Container creation from image
- Process injection into existing container
- Container pause and resume
- Container checkpoint and migration

**VM Support**:
- Hypervisor service using hardware virtualization
- Device passthrough via IOMMU
- Virtual device emulation in user space

**Success Criteria**: Multiple containers run simultaneously with isolation. Resource limits enforced. Container checkpoint completes in seconds. Virtual machine runs legacy OS.

---

### Stage 13: Power Management

**Objective**: Optimize power consumption without sacrificing responsiveness.

**Components**:

**CPU Frequency Scaling**:
- Dynamic adjustment based on load
- Integration with scheduler for intelligent scaling decisions
- Per-core frequency control on supported hardware

**Sleep States**:
- Idle CPU cores enter low-power states
- Wake on timer or interrupt
- Coordinated sleep across all cores

**Device Power Management**:
- Driver-controlled device power states
- Suspend and resume protocol
- Runtime power management for idle devices

**System Suspend**:
- Suspend-to-RAM (S3 state)
- Suspend-to-disk (hibernation)
- Fast resume with state preservation

**Success Criteria**: Idle laptop achieves multiple hours battery life. Resume from suspend completes in under one second. CPU frequency scales appropriately with load.

---

### Stage 14: Debugging and Observability

**Objective**: Provide comprehensive system introspection and debugging tools.

**Components**:

**System Call Tracing**:
- Trace all IPC messages with capability filtering
- Record timing and data for performance analysis
- Minimal overhead when not actively tracing

**Capability Audit**:
- Track capability creation, transfer, and destruction
- Identify capability leak sources
- Enforce mandatory access control policies

**Performance Counters**:
- CPU performance counter access via capability
- Per-thread and per-process performance metrics
- System-wide statistics aggregation

**Debug Services**:
- Kernel debugger for crash analysis
- User-space debugger with ptrace equivalent
- Core dump generation with capability preservation

**Success Criteria**: Trace all filesystem operations for application. Identify process holding excessive capabilities. Debug userspace driver with breakpoints and variable inspection.

---

### Stage 15: Security Hardening and Formal Verification

**Objective**: Achieve high assurance through verification and hardening.

**Components**:

**Formal Verification**:
- Specify kernel invariants formally
- Prove critical properties (memory isolation, capability unforgability)
- Automated theorem proving for core mechanisms

**Security Audit**:
- Third-party code review of kernel
- Fuzzing of IPC and capability interfaces
- Penetration testing of isolation boundaries

**Hardening Features**:
- Kernel address space layout randomization
- Stack canaries in kernel code
- Control flow integrity enforcement
- Capability-based exploit mitigation

**Certification**:
- Common Criteria evaluation
- FIPS 140-3 compliance for cryptographic components
- Safety-critical certification for real-time variant

**Success Criteria**: Formal proof of memory isolation property. Zero kernel crashes in 1000-hour fuzz testing. Independent security audit finds no critical vulnerabilities.

---

## Conclusion

This architecture specification defines a microkernel operating system that addresses fundamental security and reliability limitations of monolithic designs while maintaining ecosystem compatibility.

The development roadmap proceeds incrementally from bare metal boot to full-featured OS, with each stage building on verified foundations.

The Rust implementation ensures memory safety at the language level, eliminating entire vulnerability classes.

The result is a system suitable for deployment in security-critical, safety-critical, and general-purpose computing environments.
